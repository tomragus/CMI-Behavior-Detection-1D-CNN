{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10773bfb",
   "metadata": {},
   "source": [
    "Testing the inference function!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc08ee",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9d078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Note: RobustScaler is not needed here as we LOAD the fitted one\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. LOAD YOUR FITTED SCALER (Do this once, outside the function)\n",
    "# Ensure you saved this after training using joblib.dump(scaler, 'scaler.pkl')\n",
    "# scaler = joblib.load('/kaggle/working/scaler.pkl') \n",
    "\n",
    "def process_single_sequence(df_seq: pd.DataFrame) -> np.ndarray:\n",
    "    df_seq = df_seq.copy()\n",
    "\n",
    "    df_seq.drop_duplicates(inplace=True)\n",
    "\n",
    "    # replace -1 and 0 with NaN in TOF columns\n",
    "    tof_cols = [c for c in df_seq.columns if c.startswith('tof_')]\n",
    "    if tof_cols:\n",
    "        df_seq[tof_cols] = df_seq[tof_cols].replace([-1, 0], np.nan)\n",
    "\n",
    "    # impute missing rot values with 0\n",
    "    rots = [\"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\"]\n",
    "    # Check if columns exist before accessing\n",
    "    existing_rots = [c for c in rots if c in df_seq.columns]\n",
    "    if existing_rots:\n",
    "        df_seq[existing_rots] = df_seq[existing_rots].fillna(0)\n",
    "\n",
    "    # fill missing temperature data with average by column (Constant 0 strategy)\n",
    "    thms = [\"thm_1\", \"thm_2\", \"thm_3\", \"thm_4\", \"thm_5\"]\n",
    "    existing_thms = [c for c in thms if c in df_seq.columns]\n",
    "    if existing_thms:\n",
    "        # No need for SimpleImputer object for constant fill\n",
    "        df_seq[existing_thms] = df_seq[existing_thms].fillna(0)\n",
    "\n",
    "    # fill all NaNs in time-of-flight (ToF) columns with 400\n",
    "    if tof_cols:\n",
    "        df_seq[tof_cols] = df_seq[tof_cols].fillna(400)\n",
    "\n",
    "    # linear interpolation for missing acceleration data\n",
    "    acc_cols = [\"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "    if all(c in df_seq.columns for c in acc_cols):\n",
    "        df_seq[acc_cols] = df_seq[acc_cols].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # drop unnecessary columns (Safely)\n",
    "    columns_to_drop = ['row_id', 'sequence_type', 'sequence_counter', 'subject', 'orientation', 'behavior', 'phase']\n",
    "    df_seq = df_seq.drop(columns=[col for col in columns_to_drop if col in df_seq.columns])\n",
    "    \n",
    "    new_features_df = calculate_sequence_features(df_seq)\n",
    "    \n",
    "    # Concatenate\n",
    "    df_processed = pd.concat([df_seq, new_features_df], axis=1)\n",
    "    \n",
    "    # Reorder columns to match training\n",
    "    NEW_IMU_FEATURES = [\n",
    "        'lin_acc_x', 'lin_acc_y', 'lin_acc_z',\n",
    "        'acc_mag', 'lin_acc_mag', \n",
    "        'acc_mag_jerk', 'lin_acc_mag_jerk', \n",
    "        'rot_angle', 'angular_distance', 'rot_angle_vel', \n",
    "        'angular_vel_x', 'angular_vel_y', 'angular_vel_z'\n",
    "    ]\n",
    "    \n",
    "    id_cols = ['sequence_id']\n",
    "    if 'gesture' in df_processed.columns:\n",
    "        id_cols.append('gesture')\n",
    "    \n",
    "    original_cols = [\n",
    "        col for col in df_processed.columns \n",
    "        if col not in id_cols and col not in NEW_IMU_FEATURES\n",
    "    ]\n",
    "    \n",
    "    final_order = id_cols + NEW_IMU_FEATURES + original_cols\n",
    "    # Ensure only valid columns are selected\n",
    "    final_order = [c for c in final_order if c in df_processed.columns]\n",
    "    \n",
    "    df_processed = df_processed[final_order]\n",
    "\n",
    "    # --- C. SCALING ---\n",
    "    # ðŸ’¡ IMPORTANT: Load the global scaler. Do NOT fit() a new one here.\n",
    "    cols_to_exclude = ['sequence_id', 'gesture'] # exclude ID and Target if present\n",
    "    cols_to_scale = [c for c in df_processed.columns if c not in cols_to_exclude]\n",
    "    \n",
    "    # Transform using the PRE-FITTED scaler\n",
    "    df_processed[cols_to_scale] = scaler.transform(df_processed[cols_to_scale])\n",
    "\n",
    "    # --- D. PADDING ---\n",
    "    # Drop IDs before padding if the model doesn't expect them\n",
    "    if 'sequence_id' in df_processed.columns:\n",
    "        df_processed = df_processed.drop(columns=['sequence_id'])\n",
    "    if 'gesture' in df_processed.columns:\n",
    "        df_processed = df_processed.drop(columns=['gesture'])\n",
    "        \n",
    "    # Wrap in list to create a batch of size 1: [Sequence_Array]\n",
    "    padded_sequences = pad_sequences([df_processed.values], \n",
    "                                     padding='post', \n",
    "                                     dtype='float32', \n",
    "                                     maxlen=700)\n",
    "    \n",
    "    # Result shape is now (1, 700, 344)\n",
    "    return padded_sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
